
# Data Analytics Lab at Rice University

> ðŸš§ work in progress...

- [x] Add recent discussions.
- [x] Add selected recent work titles.
- [ ] Add author information.
- [ ] Add tl;dr.
- [ ] Add repo links.

---

## Recent Discussions



> We believe in having open conversations for better scientific discourse. Here are some recent posts related to our work on social media (by us or a third party), where we find many of such discussions quite insightful. OpenReview offers an excellent place to digest papers from a non-author perspective; social media allows us to do exactly that for preprints.
> We'd try our best to engage in such posts. Of course, you are always welcome to email us or open up an issue anytime.


[[r/MachineLearning](https://www.reddit.com/r/MachineLearning/comments/1ap3b65/), [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1ap3bkt/), [Twitter/X](), [LinkedIn](https://www.linkedin.com/posts/shaochen-henry-zhong-96a941249_kv-cache-is-huge-and-bottlenecks-llm-inference-activity-7162844534454824960-8IJ3)] KV Cache is huge and bottlenecks LLM inference. We quantize them to 2bit in a finetuning-free + plug-and-play fashion. `authors`

[[r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/18x8g6c/llm_maybe_longlm_selfextend_llm_context_window/), [Twitter/X #1](https://x.com/cwolferesearch/status/1748393116338409890?s=20) [#2](https://x.com/arankomatsuzaki/status/1742367971857883383?s=20) [#3](https://x.com/rohanpaul_ai/status/1751884202877042956?s=20) [#4](https://x.com/_akhaliq/status/1742371015362052461?s=20), [kexue.fm](https://kexue.fm/archives/9948)] LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning. `third-party`





---
## Selected Recent Work

> Some recent work from us that might worth your attention.

### Prepints

KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache [[paper]](https://arxiv.org/abs/2402.02750)`llm` `efficiency`

Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt [[paper]](https://arxiv.org/abs/2305.11186) `llm` `efficiency` 

LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning [[paper]](https://arxiv.org/abs/2401.01325)`llm` `long context` 




GrowLength: Accelerating LLMs Pretraining by Progressively Growing Training Length [[paper]](https://arxiv.org/abs/2310.00576) `llm` `long context` 

LETA: Learning Transferable Attribution for Generic Vision Explainer [[paper]](https://arxiv.org/abs/2312.15359)`vision` `xai` 

Large Language Models As Faithful Explainers [[paper]](https://arxiv.org/abs/2402.04678) `llm` `xai` 


On the Equivalence of Graph Convolution and Mixup [[paper]](https://arxiv.org/abs/2310.00183) `graph` 

Chasing Fairness in Graphs: A GNN Architecture Perspective [[paper]](https://arxiv.org/abs/2312.12369) `graph` `trustworthy` 

Editable Graph Neural Network for Node Classifications [[paper]](https://arxiv.org/abs/2305.15529) `graph` `trustworthy`

Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond [[paper]](https://arxiv.org/abs/2304.13712) `llm` `survey`

Data-centric Artificial Intelligence: A Survey [[paper]](https://arxiv.org/abs/2303.10158) `dcai` `survey`

---   

### CACM 2024

The Science of Detecting LLM-Generated Texts `llm` `security`

---

### ICLR 2024

FFB: A Fair Fairness Benchmark for In-Processing Group Fairness Methods `graph` `benchmark` `trustworthy` 

---   

### NeurIPS 2023

Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model `llm` `efficiency` 

One Less Reason for Filter Pruning: Gaining Free Adversarial Robustness with Structured Grouped Kernel Pruning `efficiency` `trustworthy` `security` 

Setting the Trap: Capturing and Defeating Backdoors in Pretrained Language Models through Honeypots `llm` `security` 

Chasing Fairness Under Distribution Shift: A Model Weight Perturbation Approach `trustworthy` 

Fair Graph Distillation `graph` `trustworthy` 

---   

### ICDM 2023 
Double wins: Boosting accuracy and efficiency of graph neural networks by reliable knowledge distillation `graph` `efficiency` 


---   

### AMIA 2023

LLM for Patient-Trial Matching: Privacy-Aware Data Augmentation Towards Better Performance and Generalizability `healthcare` `llm` 

Multi-Task Learning for Post-transplant Cause of Death Analysis: A Case Study on Liver Transplant `healthcare` 

---   

### CIKM 2023

DiscoverPath: A Knowledge Refinement and Retrieval System for Interdisciplinarity on Biomedical Research `healthcare` 

Exposing Model Theft: A Robust and Transferable Watermark for Thwarting Model Extraction Attacks `security` 

---   

### SDM 2023

Data-centric AI: Perspectives and Challenges `survey` `dcai` 

Context-aware Domain Adaptation for Time Series Anomaly Detection `time series` 

Adaptive Label Smoothing To Regularize Large-Scale Graph Training `graph` 

---   

### ICML 2023

DIVISION: Memory Efficient Training via Dual Activation Precision `vision` `efficiency` 

RSC: Accelerating Graph Neural Networks Training via Randomized Sparse Computations `graph` `efficiency` 

PME: pruning-based multi-size embedding for recommender systems `recsys` `efficiency` 

---   

### MLSys 2023

Pre-trained Neural Cost Models for Efficient Embedding Table Sharding in Deep Learning Recommendation Models `recsys` `efficiency` 

---   

### ICLR 2023

CoRTX: Contrastive Framework for Real-time Explanation `xai` 

MLPInit: Embarrassingly Simple GNN Training Acceleration with MLP Initialization `graph` `efficiency` 



